{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.53 GiB of which 12.44 MiB is free. Process 1955584 has 10.39 GiB memory in use. Process 2319473 has 10.39 GiB memory in use. Process 2951439 has 4.47 GiB memory in use. Process 2956205 has 4.47 GiB memory in use. Process 2959648 has 10.36 GiB memory in use. Process 2962038 has 3.96 GiB memory in use. Including non-PyTorch memory, this process has 1.11 GiB memory in use. Process 3060104 has 840.00 MiB memory in use. Process 3060974 has 840.00 MiB memory in use. Process 3061392 has 698.00 MiB memory in use. Of the allocated memory 765.83 MiB is allocated by PyTorch, and 70.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Path/To/results/models/Food101-best-20240505-013323.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/multimodal-classification/UniS-MMC/.venv_mm/lib/python3.8/site-packages/torch/serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/multimodal-classification/UniS-MMC/.venv_mm/lib/python3.8/site-packages/torch/serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1443\u001b[0m )\n",
      "File \u001b[0;32m~/multimodal-classification/UniS-MMC/.venv_mm/lib/python3.8/site-packages/torch/serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1408\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/multimodal-classification/UniS-MMC/.venv_mm/lib/python3.8/site-packages/torch/serialization.py:1382\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1382\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1383\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1384\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1387\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/multimodal-classification/UniS-MMC/.venv_mm/lib/python3.8/site-packages/torch/serialization.py:391\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 391\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/multimodal-classification/UniS-MMC/.venv_mm/lib/python3.8/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/multimodal-classification/UniS-MMC/.venv_mm/lib/python3.8/site-packages/torch/_utils.py:115\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_type(indices, values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.53 GiB of which 12.44 MiB is free. Process 1955584 has 10.39 GiB memory in use. Process 2319473 has 10.39 GiB memory in use. Process 2951439 has 4.47 GiB memory in use. Process 2956205 has 4.47 GiB memory in use. Process 2959648 has 10.36 GiB memory in use. Process 2962038 has 3.96 GiB memory in use. Including non-PyTorch memory, this process has 1.11 GiB memory in use. Process 3060104 has 840.00 MiB memory in use. Process 3060974 has 840.00 MiB memory in use. Process 3061392 has 698.00 MiB memory in use. Of the allocated memory 765.83 MiB is allocated by PyTorch, and 70.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda:2\"\n",
    "model_path = '../Path/To/results/models/Food101-best-20240505-013323.pth'\n",
    "model = torch.load(model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'named_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m names,param \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_parameters\u001b[49m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(names)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'named_parameters'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'text_encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_encoder\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'text_encoder'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['image_encoder.model.embeddings.cls_token', 'image_encoder.model.embeddings.position_embeddings', 'image_encoder.model.embeddings.patch_embeddings.projection.weight', 'image_encoder.model.embeddings.patch_embeddings.projection.bias', 'image_encoder.model.encoder.layer.0.attention.attention.query.weight', 'image_encoder.model.encoder.layer.0.attention.attention.query.bias', 'image_encoder.model.encoder.layer.0.attention.attention.key.weight', 'image_encoder.model.encoder.layer.0.attention.attention.key.bias', 'image_encoder.model.encoder.layer.0.attention.attention.value.weight', 'image_encoder.model.encoder.layer.0.attention.attention.value.bias', 'image_encoder.model.encoder.layer.0.attention.output.dense.weight', 'image_encoder.model.encoder.layer.0.attention.output.dense.bias', 'image_encoder.model.encoder.layer.0.intermediate.dense.weight', 'image_encoder.model.encoder.layer.0.intermediate.dense.bias', 'image_encoder.model.encoder.layer.0.output.dense.weight', 'image_encoder.model.encoder.layer.0.output.dense.bias', 'image_encoder.model.encoder.layer.0.layernorm_before.weight', 'image_encoder.model.encoder.layer.0.layernorm_before.bias', 'image_encoder.model.encoder.layer.0.layernorm_after.weight', 'image_encoder.model.encoder.layer.0.layernorm_after.bias', 'image_encoder.model.encoder.layer.1.attention.attention.query.weight', 'image_encoder.model.encoder.layer.1.attention.attention.query.bias', 'image_encoder.model.encoder.layer.1.attention.attention.key.weight', 'image_encoder.model.encoder.layer.1.attention.attention.key.bias', 'image_encoder.model.encoder.layer.1.attention.attention.value.weight', 'image_encoder.model.encoder.layer.1.attention.attention.value.bias', 'image_encoder.model.encoder.layer.1.attention.output.dense.weight', 'image_encoder.model.encoder.layer.1.attention.output.dense.bias', 'image_encoder.model.encoder.layer.1.intermediate.dense.weight', 'image_encoder.model.encoder.layer.1.intermediate.dense.bias', 'image_encoder.model.encoder.layer.1.output.dense.weight', 'image_encoder.model.encoder.layer.1.output.dense.bias', 'image_encoder.model.encoder.layer.1.layernorm_before.weight', 'image_encoder.model.encoder.layer.1.layernorm_before.bias', 'image_encoder.model.encoder.layer.1.layernorm_after.weight', 'image_encoder.model.encoder.layer.1.layernorm_after.bias', 'image_encoder.model.encoder.layer.2.attention.attention.query.weight', 'image_encoder.model.encoder.layer.2.attention.attention.query.bias', 'image_encoder.model.encoder.layer.2.attention.attention.key.weight', 'image_encoder.model.encoder.layer.2.attention.attention.key.bias', 'image_encoder.model.encoder.layer.2.attention.attention.value.weight', 'image_encoder.model.encoder.layer.2.attention.attention.value.bias', 'image_encoder.model.encoder.layer.2.attention.output.dense.weight', 'image_encoder.model.encoder.layer.2.attention.output.dense.bias', 'image_encoder.model.encoder.layer.2.intermediate.dense.weight', 'image_encoder.model.encoder.layer.2.intermediate.dense.bias', 'image_encoder.model.encoder.layer.2.output.dense.weight', 'image_encoder.model.encoder.layer.2.output.dense.bias', 'image_encoder.model.encoder.layer.2.layernorm_before.weight', 'image_encoder.model.encoder.layer.2.layernorm_before.bias', 'image_encoder.model.encoder.layer.2.layernorm_after.weight', 'image_encoder.model.encoder.layer.2.layernorm_after.bias', 'image_encoder.model.encoder.layer.3.attention.attention.query.weight', 'image_encoder.model.encoder.layer.3.attention.attention.query.bias', 'image_encoder.model.encoder.layer.3.attention.attention.key.weight', 'image_encoder.model.encoder.layer.3.attention.attention.key.bias', 'image_encoder.model.encoder.layer.3.attention.attention.value.weight', 'image_encoder.model.encoder.layer.3.attention.attention.value.bias', 'image_encoder.model.encoder.layer.3.attention.output.dense.weight', 'image_encoder.model.encoder.layer.3.attention.output.dense.bias', 'image_encoder.model.encoder.layer.3.intermediate.dense.weight', 'image_encoder.model.encoder.layer.3.intermediate.dense.bias', 'image_encoder.model.encoder.layer.3.output.dense.weight', 'image_encoder.model.encoder.layer.3.output.dense.bias', 'image_encoder.model.encoder.layer.3.layernorm_before.weight', 'image_encoder.model.encoder.layer.3.layernorm_before.bias', 'image_encoder.model.encoder.layer.3.layernorm_after.weight', 'image_encoder.model.encoder.layer.3.layernorm_after.bias', 'image_encoder.model.encoder.layer.4.attention.attention.query.weight', 'image_encoder.model.encoder.layer.4.attention.attention.query.bias', 'image_encoder.model.encoder.layer.4.attention.attention.key.weight', 'image_encoder.model.encoder.layer.4.attention.attention.key.bias', 'image_encoder.model.encoder.layer.4.attention.attention.value.weight', 'image_encoder.model.encoder.layer.4.attention.attention.value.bias', 'image_encoder.model.encoder.layer.4.attention.output.dense.weight', 'image_encoder.model.encoder.layer.4.attention.output.dense.bias', 'image_encoder.model.encoder.layer.4.intermediate.dense.weight', 'image_encoder.model.encoder.layer.4.intermediate.dense.bias', 'image_encoder.model.encoder.layer.4.output.dense.weight', 'image_encoder.model.encoder.layer.4.output.dense.bias', 'image_encoder.model.encoder.layer.4.layernorm_before.weight', 'image_encoder.model.encoder.layer.4.layernorm_before.bias', 'image_encoder.model.encoder.layer.4.layernorm_after.weight', 'image_encoder.model.encoder.layer.4.layernorm_after.bias', 'image_encoder.model.encoder.layer.5.attention.attention.query.weight', 'image_encoder.model.encoder.layer.5.attention.attention.query.bias', 'image_encoder.model.encoder.layer.5.attention.attention.key.weight', 'image_encoder.model.encoder.layer.5.attention.attention.key.bias', 'image_encoder.model.encoder.layer.5.attention.attention.value.weight', 'image_encoder.model.encoder.layer.5.attention.attention.value.bias', 'image_encoder.model.encoder.layer.5.attention.output.dense.weight', 'image_encoder.model.encoder.layer.5.attention.output.dense.bias', 'image_encoder.model.encoder.layer.5.intermediate.dense.weight', 'image_encoder.model.encoder.layer.5.intermediate.dense.bias', 'image_encoder.model.encoder.layer.5.output.dense.weight', 'image_encoder.model.encoder.layer.5.output.dense.bias', 'image_encoder.model.encoder.layer.5.layernorm_before.weight', 'image_encoder.model.encoder.layer.5.layernorm_before.bias', 'image_encoder.model.encoder.layer.5.layernorm_after.weight', 'image_encoder.model.encoder.layer.5.layernorm_after.bias', 'image_encoder.model.encoder.layer.6.attention.attention.query.weight', 'image_encoder.model.encoder.layer.6.attention.attention.query.bias', 'image_encoder.model.encoder.layer.6.attention.attention.key.weight', 'image_encoder.model.encoder.layer.6.attention.attention.key.bias', 'image_encoder.model.encoder.layer.6.attention.attention.value.weight', 'image_encoder.model.encoder.layer.6.attention.attention.value.bias', 'image_encoder.model.encoder.layer.6.attention.output.dense.weight', 'image_encoder.model.encoder.layer.6.attention.output.dense.bias', 'image_encoder.model.encoder.layer.6.intermediate.dense.weight', 'image_encoder.model.encoder.layer.6.intermediate.dense.bias', 'image_encoder.model.encoder.layer.6.output.dense.weight', 'image_encoder.model.encoder.layer.6.output.dense.bias', 'image_encoder.model.encoder.layer.6.layernorm_before.weight', 'image_encoder.model.encoder.layer.6.layernorm_before.bias', 'image_encoder.model.encoder.layer.6.layernorm_after.weight', 'image_encoder.model.encoder.layer.6.layernorm_after.bias', 'image_encoder.model.encoder.layer.7.attention.attention.query.weight', 'image_encoder.model.encoder.layer.7.attention.attention.query.bias', 'image_encoder.model.encoder.layer.7.attention.attention.key.weight', 'image_encoder.model.encoder.layer.7.attention.attention.key.bias', 'image_encoder.model.encoder.layer.7.attention.attention.value.weight', 'image_encoder.model.encoder.layer.7.attention.attention.value.bias', 'image_encoder.model.encoder.layer.7.attention.output.dense.weight', 'image_encoder.model.encoder.layer.7.attention.output.dense.bias', 'image_encoder.model.encoder.layer.7.intermediate.dense.weight', 'image_encoder.model.encoder.layer.7.intermediate.dense.bias', 'image_encoder.model.encoder.layer.7.output.dense.weight', 'image_encoder.model.encoder.layer.7.output.dense.bias', 'image_encoder.model.encoder.layer.7.layernorm_before.weight', 'image_encoder.model.encoder.layer.7.layernorm_before.bias', 'image_encoder.model.encoder.layer.7.layernorm_after.weight', 'image_encoder.model.encoder.layer.7.layernorm_after.bias', 'image_encoder.model.encoder.layer.8.attention.attention.query.weight', 'image_encoder.model.encoder.layer.8.attention.attention.query.bias', 'image_encoder.model.encoder.layer.8.attention.attention.key.weight', 'image_encoder.model.encoder.layer.8.attention.attention.key.bias', 'image_encoder.model.encoder.layer.8.attention.attention.value.weight', 'image_encoder.model.encoder.layer.8.attention.attention.value.bias', 'image_encoder.model.encoder.layer.8.attention.output.dense.weight', 'image_encoder.model.encoder.layer.8.attention.output.dense.bias', 'image_encoder.model.encoder.layer.8.intermediate.dense.weight', 'image_encoder.model.encoder.layer.8.intermediate.dense.bias', 'image_encoder.model.encoder.layer.8.output.dense.weight', 'image_encoder.model.encoder.layer.8.output.dense.bias', 'image_encoder.model.encoder.layer.8.layernorm_before.weight', 'image_encoder.model.encoder.layer.8.layernorm_before.bias', 'image_encoder.model.encoder.layer.8.layernorm_after.weight', 'image_encoder.model.encoder.layer.8.layernorm_after.bias', 'image_encoder.model.encoder.layer.9.attention.attention.query.weight', 'image_encoder.model.encoder.layer.9.attention.attention.query.bias', 'image_encoder.model.encoder.layer.9.attention.attention.key.weight', 'image_encoder.model.encoder.layer.9.attention.attention.key.bias', 'image_encoder.model.encoder.layer.9.attention.attention.value.weight', 'image_encoder.model.encoder.layer.9.attention.attention.value.bias', 'image_encoder.model.encoder.layer.9.attention.output.dense.weight', 'image_encoder.model.encoder.layer.9.attention.output.dense.bias', 'image_encoder.model.encoder.layer.9.intermediate.dense.weight', 'image_encoder.model.encoder.layer.9.intermediate.dense.bias', 'image_encoder.model.encoder.layer.9.output.dense.weight', 'image_encoder.model.encoder.layer.9.output.dense.bias', 'image_encoder.model.encoder.layer.9.layernorm_before.weight', 'image_encoder.model.encoder.layer.9.layernorm_before.bias', 'image_encoder.model.encoder.layer.9.layernorm_after.weight', 'image_encoder.model.encoder.layer.9.layernorm_after.bias', 'image_encoder.model.encoder.layer.10.attention.attention.query.weight', 'image_encoder.model.encoder.layer.10.attention.attention.query.bias', 'image_encoder.model.encoder.layer.10.attention.attention.key.weight', 'image_encoder.model.encoder.layer.10.attention.attention.key.bias', 'image_encoder.model.encoder.layer.10.attention.attention.value.weight', 'image_encoder.model.encoder.layer.10.attention.attention.value.bias', 'image_encoder.model.encoder.layer.10.attention.output.dense.weight', 'image_encoder.model.encoder.layer.10.attention.output.dense.bias', 'image_encoder.model.encoder.layer.10.intermediate.dense.weight', 'image_encoder.model.encoder.layer.10.intermediate.dense.bias', 'image_encoder.model.encoder.layer.10.output.dense.weight', 'image_encoder.model.encoder.layer.10.output.dense.bias', 'image_encoder.model.encoder.layer.10.layernorm_before.weight', 'image_encoder.model.encoder.layer.10.layernorm_before.bias', 'image_encoder.model.encoder.layer.10.layernorm_after.weight', 'image_encoder.model.encoder.layer.10.layernorm_after.bias', 'image_encoder.model.encoder.layer.11.attention.attention.query.weight', 'image_encoder.model.encoder.layer.11.attention.attention.query.bias', 'image_encoder.model.encoder.layer.11.attention.attention.key.weight', 'image_encoder.model.encoder.layer.11.attention.attention.key.bias', 'image_encoder.model.encoder.layer.11.attention.attention.value.weight', 'image_encoder.model.encoder.layer.11.attention.attention.value.bias', 'image_encoder.model.encoder.layer.11.attention.output.dense.weight', 'image_encoder.model.encoder.layer.11.attention.output.dense.bias', 'image_encoder.model.encoder.layer.11.intermediate.dense.weight', 'image_encoder.model.encoder.layer.11.intermediate.dense.bias', 'image_encoder.model.encoder.layer.11.output.dense.weight', 'image_encoder.model.encoder.layer.11.output.dense.bias', 'image_encoder.model.encoder.layer.11.layernorm_before.weight', 'image_encoder.model.encoder.layer.11.layernorm_before.bias', 'image_encoder.model.encoder.layer.11.layernorm_after.weight', 'image_encoder.model.encoder.layer.11.layernorm_after.bias', 'image_encoder.model.layernorm.weight', 'image_encoder.model.layernorm.bias', 'image_encoder.model.pooler.dense.weight', 'image_encoder.model.pooler.dense.bias', 'image_classfier.post_layer_1.clf.0.weight', 'image_classfier.post_layer_1.clf.0.bias', 'image_classfier.post_layer_2.clf.0.weight', 'image_classfier.post_layer_2.clf.0.bias', 'image_classfier.post_layer_3.clf.0.weight', 'image_classfier.post_layer_3.clf.0.bias', 'text_encoder.model.embeddings.position_ids', 'text_encoder.model.embeddings.word_embeddings.weight', 'text_encoder.model.embeddings.position_embeddings.weight', 'text_encoder.model.embeddings.token_type_embeddings.weight', 'text_encoder.model.embeddings.LayerNorm.weight', 'text_encoder.model.embeddings.LayerNorm.bias', 'text_encoder.model.encoder.layer.0.attention.self.query.weight', 'text_encoder.model.encoder.layer.0.attention.self.query.bias', 'text_encoder.model.encoder.layer.0.attention.self.key.weight', 'text_encoder.model.encoder.layer.0.attention.self.key.bias', 'text_encoder.model.encoder.layer.0.attention.self.value.weight', 'text_encoder.model.encoder.layer.0.attention.self.value.bias', 'text_encoder.model.encoder.layer.0.attention.output.dense.weight', 'text_encoder.model.encoder.layer.0.attention.output.dense.bias', 'text_encoder.model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.0.intermediate.dense.weight', 'text_encoder.model.encoder.layer.0.intermediate.dense.bias', 'text_encoder.model.encoder.layer.0.output.dense.weight', 'text_encoder.model.encoder.layer.0.output.dense.bias', 'text_encoder.model.encoder.layer.0.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.0.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.1.attention.self.query.weight', 'text_encoder.model.encoder.layer.1.attention.self.query.bias', 'text_encoder.model.encoder.layer.1.attention.self.key.weight', 'text_encoder.model.encoder.layer.1.attention.self.key.bias', 'text_encoder.model.encoder.layer.1.attention.self.value.weight', 'text_encoder.model.encoder.layer.1.attention.self.value.bias', 'text_encoder.model.encoder.layer.1.attention.output.dense.weight', 'text_encoder.model.encoder.layer.1.attention.output.dense.bias', 'text_encoder.model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.1.intermediate.dense.weight', 'text_encoder.model.encoder.layer.1.intermediate.dense.bias', 'text_encoder.model.encoder.layer.1.output.dense.weight', 'text_encoder.model.encoder.layer.1.output.dense.bias', 'text_encoder.model.encoder.layer.1.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.1.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.2.attention.self.query.weight', 'text_encoder.model.encoder.layer.2.attention.self.query.bias', 'text_encoder.model.encoder.layer.2.attention.self.key.weight', 'text_encoder.model.encoder.layer.2.attention.self.key.bias', 'text_encoder.model.encoder.layer.2.attention.self.value.weight', 'text_encoder.model.encoder.layer.2.attention.self.value.bias', 'text_encoder.model.encoder.layer.2.attention.output.dense.weight', 'text_encoder.model.encoder.layer.2.attention.output.dense.bias', 'text_encoder.model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.2.intermediate.dense.weight', 'text_encoder.model.encoder.layer.2.intermediate.dense.bias', 'text_encoder.model.encoder.layer.2.output.dense.weight', 'text_encoder.model.encoder.layer.2.output.dense.bias', 'text_encoder.model.encoder.layer.2.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.2.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.3.attention.self.query.weight', 'text_encoder.model.encoder.layer.3.attention.self.query.bias', 'text_encoder.model.encoder.layer.3.attention.self.key.weight', 'text_encoder.model.encoder.layer.3.attention.self.key.bias', 'text_encoder.model.encoder.layer.3.attention.self.value.weight', 'text_encoder.model.encoder.layer.3.attention.self.value.bias', 'text_encoder.model.encoder.layer.3.attention.output.dense.weight', 'text_encoder.model.encoder.layer.3.attention.output.dense.bias', 'text_encoder.model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.3.intermediate.dense.weight', 'text_encoder.model.encoder.layer.3.intermediate.dense.bias', 'text_encoder.model.encoder.layer.3.output.dense.weight', 'text_encoder.model.encoder.layer.3.output.dense.bias', 'text_encoder.model.encoder.layer.3.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.3.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.4.attention.self.query.weight', 'text_encoder.model.encoder.layer.4.attention.self.query.bias', 'text_encoder.model.encoder.layer.4.attention.self.key.weight', 'text_encoder.model.encoder.layer.4.attention.self.key.bias', 'text_encoder.model.encoder.layer.4.attention.self.value.weight', 'text_encoder.model.encoder.layer.4.attention.self.value.bias', 'text_encoder.model.encoder.layer.4.attention.output.dense.weight', 'text_encoder.model.encoder.layer.4.attention.output.dense.bias', 'text_encoder.model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.4.intermediate.dense.weight', 'text_encoder.model.encoder.layer.4.intermediate.dense.bias', 'text_encoder.model.encoder.layer.4.output.dense.weight', 'text_encoder.model.encoder.layer.4.output.dense.bias', 'text_encoder.model.encoder.layer.4.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.4.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.5.attention.self.query.weight', 'text_encoder.model.encoder.layer.5.attention.self.query.bias', 'text_encoder.model.encoder.layer.5.attention.self.key.weight', 'text_encoder.model.encoder.layer.5.attention.self.key.bias', 'text_encoder.model.encoder.layer.5.attention.self.value.weight', 'text_encoder.model.encoder.layer.5.attention.self.value.bias', 'text_encoder.model.encoder.layer.5.attention.output.dense.weight', 'text_encoder.model.encoder.layer.5.attention.output.dense.bias', 'text_encoder.model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.5.intermediate.dense.weight', 'text_encoder.model.encoder.layer.5.intermediate.dense.bias', 'text_encoder.model.encoder.layer.5.output.dense.weight', 'text_encoder.model.encoder.layer.5.output.dense.bias', 'text_encoder.model.encoder.layer.5.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.5.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.6.attention.self.query.weight', 'text_encoder.model.encoder.layer.6.attention.self.query.bias', 'text_encoder.model.encoder.layer.6.attention.self.key.weight', 'text_encoder.model.encoder.layer.6.attention.self.key.bias', 'text_encoder.model.encoder.layer.6.attention.self.value.weight', 'text_encoder.model.encoder.layer.6.attention.self.value.bias', 'text_encoder.model.encoder.layer.6.attention.output.dense.weight', 'text_encoder.model.encoder.layer.6.attention.output.dense.bias', 'text_encoder.model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.6.intermediate.dense.weight', 'text_encoder.model.encoder.layer.6.intermediate.dense.bias', 'text_encoder.model.encoder.layer.6.output.dense.weight', 'text_encoder.model.encoder.layer.6.output.dense.bias', 'text_encoder.model.encoder.layer.6.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.6.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.7.attention.self.query.weight', 'text_encoder.model.encoder.layer.7.attention.self.query.bias', 'text_encoder.model.encoder.layer.7.attention.self.key.weight', 'text_encoder.model.encoder.layer.7.attention.self.key.bias', 'text_encoder.model.encoder.layer.7.attention.self.value.weight', 'text_encoder.model.encoder.layer.7.attention.self.value.bias', 'text_encoder.model.encoder.layer.7.attention.output.dense.weight', 'text_encoder.model.encoder.layer.7.attention.output.dense.bias', 'text_encoder.model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.7.intermediate.dense.weight', 'text_encoder.model.encoder.layer.7.intermediate.dense.bias', 'text_encoder.model.encoder.layer.7.output.dense.weight', 'text_encoder.model.encoder.layer.7.output.dense.bias', 'text_encoder.model.encoder.layer.7.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.7.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.8.attention.self.query.weight', 'text_encoder.model.encoder.layer.8.attention.self.query.bias', 'text_encoder.model.encoder.layer.8.attention.self.key.weight', 'text_encoder.model.encoder.layer.8.attention.self.key.bias', 'text_encoder.model.encoder.layer.8.attention.self.value.weight', 'text_encoder.model.encoder.layer.8.attention.self.value.bias', 'text_encoder.model.encoder.layer.8.attention.output.dense.weight', 'text_encoder.model.encoder.layer.8.attention.output.dense.bias', 'text_encoder.model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.8.intermediate.dense.weight', 'text_encoder.model.encoder.layer.8.intermediate.dense.bias', 'text_encoder.model.encoder.layer.8.output.dense.weight', 'text_encoder.model.encoder.layer.8.output.dense.bias', 'text_encoder.model.encoder.layer.8.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.8.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.9.attention.self.query.weight', 'text_encoder.model.encoder.layer.9.attention.self.query.bias', 'text_encoder.model.encoder.layer.9.attention.self.key.weight', 'text_encoder.model.encoder.layer.9.attention.self.key.bias', 'text_encoder.model.encoder.layer.9.attention.self.value.weight', 'text_encoder.model.encoder.layer.9.attention.self.value.bias', 'text_encoder.model.encoder.layer.9.attention.output.dense.weight', 'text_encoder.model.encoder.layer.9.attention.output.dense.bias', 'text_encoder.model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.9.intermediate.dense.weight', 'text_encoder.model.encoder.layer.9.intermediate.dense.bias', 'text_encoder.model.encoder.layer.9.output.dense.weight', 'text_encoder.model.encoder.layer.9.output.dense.bias', 'text_encoder.model.encoder.layer.9.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.9.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.10.attention.self.query.weight', 'text_encoder.model.encoder.layer.10.attention.self.query.bias', 'text_encoder.model.encoder.layer.10.attention.self.key.weight', 'text_encoder.model.encoder.layer.10.attention.self.key.bias', 'text_encoder.model.encoder.layer.10.attention.self.value.weight', 'text_encoder.model.encoder.layer.10.attention.self.value.bias', 'text_encoder.model.encoder.layer.10.attention.output.dense.weight', 'text_encoder.model.encoder.layer.10.attention.output.dense.bias', 'text_encoder.model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.10.intermediate.dense.weight', 'text_encoder.model.encoder.layer.10.intermediate.dense.bias', 'text_encoder.model.encoder.layer.10.output.dense.weight', 'text_encoder.model.encoder.layer.10.output.dense.bias', 'text_encoder.model.encoder.layer.10.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.10.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.11.attention.self.query.weight', 'text_encoder.model.encoder.layer.11.attention.self.query.bias', 'text_encoder.model.encoder.layer.11.attention.self.key.weight', 'text_encoder.model.encoder.layer.11.attention.self.key.bias', 'text_encoder.model.encoder.layer.11.attention.self.value.weight', 'text_encoder.model.encoder.layer.11.attention.self.value.bias', 'text_encoder.model.encoder.layer.11.attention.output.dense.weight', 'text_encoder.model.encoder.layer.11.attention.output.dense.bias', 'text_encoder.model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder.model.encoder.layer.11.intermediate.dense.weight', 'text_encoder.model.encoder.layer.11.intermediate.dense.bias', 'text_encoder.model.encoder.layer.11.output.dense.weight', 'text_encoder.model.encoder.layer.11.output.dense.bias', 'text_encoder.model.encoder.layer.11.output.LayerNorm.weight', 'text_encoder.model.encoder.layer.11.output.LayerNorm.bias', 'text_encoder.model.pooler.dense.weight', 'text_encoder.model.pooler.dense.bias', 'text_classfier.post_layer_1.clf.0.weight', 'text_classfier.post_layer_1.clf.0.bias', 'text_classfier.post_layer_2.clf.0.weight', 'text_classfier.post_layer_2.clf.0.bias', 'text_classfier.post_layer_3.clf.0.weight', 'text_classfier.post_layer_3.clf.0.bias', 'mm_classfier.post_layer_1.clf.0.weight', 'mm_classfier.post_layer_1.clf.0.bias', 'mm_classfier.post_layer_2.clf.0.weight', 'mm_classfier.post_layer_2.clf.0.bias', 'mm_classfier.post_layer_3.clf.0.weight', 'mm_classfier.post_layer_3.clf.0.bias'])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (M3CoL)",
   "language": "python",
   "name": ".venv_mm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
